---
title: "Comparison of the precison of flood frequency estimators"
author: "Martin Durocher"
date: "02/12/2019"
output: html_document
---


```{r setup, include=FALSE}
## Run from project directory
knitr::opts_chunk$set(echo = FALSE)
suppressPackageStartupMessages(library(floodnetRfa))
source('config')
```

## Introduction

The R-package `FloodnetRfa` provides functions to perform Regional Frequency Analysis (RFA) following the guidelines of Floodnet.
The script `atsite_script.R` and `RFA_script.R` performed the analysis on the 1114 stations using at-site and regional methods.
This document mainly compares the precision of the flood quantile estimates performed by the different functions of `floodnetRfa`, and
it also investigates the heterogeneity of the pooling groups.

## Evaluation criteria

The precision of two estimation methods is compared based on the Confidence Interval Lengths (CIL) defined as the difference between its upper and lower bounds on the interval with a probability coverage of 95%. 
If $r_i$ is the CIL of a reference method and $s_i$ is the CIL of an alternative method, we define the relative difference of precision (RDP) as $(s_{i}-r_i)/r_i$. 
The average RDP  summarizes the overall difference in precision between two estimators for a group of stations.
Such differences in the RDP may be small and solely due to the random nature of the experiment.
To assess the significance of the difference in precision, we evaluate the p-value of the paired Wilcoxon signed-rank test (WILCOX). 
This test assumes the comparison of symmetric distributions around the median, which we found to be a reasonable hypothesis after a logarithm transformation.
In particular, we consider a p-value below the significance level of `0.05` as an indication of a significant difference in precision.

Another way of comparing two estimation methods consists of examining the percentage of stations (PCT) where the CIL of the alternative estimator is lower than the one of the reference estimator ($s_i < r_i$). 
In this case, the p-value of a binomial test (PVAL) allows verifying the hypothesis that the PCT is significantly different from 50%.
One drawback of this approach is that we do not consider the magnitude of the difference.

## At-site Flood Frequency Analysis

### FloodnetAmax vs FloodnetPot

In a first step, we investigated at-site frequency analysis for both Annual Maximum discharge (AMAX) and Peaks Over Threshold (POT).
In other words, we compare the precision of the function `FloodnetAmax` versus `FloodnetPot,` where the threshold of the POT model is automatically selected.
The results are produced by the script `atsite_script.R`, which generates summary graphics in the CACHE folder. 
An example is presented at the end of the document. 

First, we noticed that in some situations, POT performed poorly. 
Therefore, we examine the stations where the CIL of the POT estimates is more than ten times the one of AMAX, as these situations represent clear cases where POT was inappropriate.
Such situations occur for 16 stations, of which 13 are in region 3.

Table 1 reports the evaluation criteria described earlier.
Both the WILCOX and Binomial tests suggest a significant difference in favour of POT for Q100.
One exception is region three, where WILCOX indicates that the difference is not significant. 
Regions 3 have a positive RDP, but a PCT higher than 50% for POT,
which indicates that POT is more precise in most situations, but tends to be substantially less precise in the others. 
According to the binomial test, the difference in precision for Q10 is not significant for regions 1, 4, 5 and 6.
On the opposite, POT is more often more precise in region 3.

Note that for region 3, several stations are seasonal stations that don't record streamflow all year long.
In these cases, we assume that the missing days were values below the threshold. 
Such an assumption can be a reasonable compromise. 
However, we would recommend using AMAX to avoid relying on such a strong assumption.
Overall, the results suggest no clear evidence for Q10 that one method is generally more precise than the other. 
On the other hand, POT appears more often more precise when considering Q100.

```{r}

supreg <- GAUGEDSITES$supreg_km6

## Merged data from atsite and RFA
xa <- read.csv(gzfile('RFA/atsite_flood_quantiles.csv.gz'))
xd <- read.csv(gzfile('RFA/rfa_flood_quantiles.csv.gz'))
xd <- rbind(xa,xd)

xd <- xd[order(as.character(xd[,1]), 
							 as.character(xd[,2]),
							 as.character(xd[,4]),
							 as.character(xd[,5])),]

## Function to extract the information from xd
GetXd <- function(m, v, p){
  xx <- with(xd, xd[method == m & variable == v & period == p, -c(2:3,5)])
  unlist(split(xx[, 3], xx[,2]))
}

GetStat <- function(m,p){
	var.lst <- unique(with(xd,xd[method == m & period == p, 5]))
	ans <- sapply(var.lst, function(z) GetXd(m,z,p))
	ans <- as.data.frame(ans)
	colnames(ans) <- as.character(var.lst)
	rownames(ans) <- NULL
	
	ans$rg <- ans$upper - ans$lower
	
	if(m %in% c('pool_amax_cv','pool_pot_shape'))
	  ans$hat <- ans$qmean
	else
		ans$hat <- ans$quantile
	
	ans$sk <- ans$rg / ans$hat
		
	return(ans)
}

## Binomial test
btest <- function(z){
  binom.test(sum(z), length(z), p = .5)$p.value
}

## Return comparison statistics
compare <- function(a, b, id){

	## Filter stations
	a <- a[id]
	b <- b[id]
	supreg0 <- supreg[id]
	
	## differential statistics
	dif <- b/a-1
	p <- a > b 

	## Wilcox
	a0 <- split(log(a),supreg0)
	b0 <- split(log(b),supreg0)
	wcox <- mapply(function(z1,z2) wilcox.test(z1,z2, paired = TRUE)$p.value,
				         a0, b0)
	
  ans <- 
		data.frame(RDP = tapply(dif, supreg0, mean),
							 WILCOX = wcox,
							 PCT = tapply(p, supreg0, mean),
							 PVAL = tapply(p, supreg0, btest))
	
	tot <- c(mean(dif), wilcox.test(a, b, paired = TRUE)$p.value, 
					 mean(p), btest(p))
	
	t(round(rbind(ans, tot = tot),3)) *100
}

amax10 <- GetStat('amax', 10)
amax100 <- GetStat('amax', 100)
pot10 <- GetStat('pot', 10)
pot100 <- GetStat('pot', 100)

station.lst <- (pot10$rg / amax10$rg < 10) & (pot100$rg / amax100$rg < 10)

```

**Table 1: Comparison of estimated precision between FloodnetAmax and FloodnetPot.** 

**_Q10_**
```{r echo = FALSE}
compare(amax10$rg, pot10$rg, id = station.lst)
```

**_Q100_**
```{r echo = FALSE}
compare(amax100$rg, pot100$rg, id = station.lst)
```

## Regional frequency analysis

### FloodnetAmax vs FloodnetPool

In this section, we compare the estimation of the at-site (`FloodnetAmax`) and regional method (`FloodnetPool`) using the L-moments algorithm with annual maximum streamflow.
The Regional Frequency Analysis (RFA) is using a pooling group approach, starting with a group of 25 stations that we update by removing the most heterogeneous stations.

Using the 1114 stations, Table 2 compares the CIL of the at-site and regional methods.
The criteria show that RFA improves the precision of the estimates for a large proportion of the stations.
For Q100, RFA leads to more precise estimates in 87.5% of the stations.
The region with the lowest improvement is region 4, with an RDP of -6.9%, while across Canada, we observe an RDP of -30.9%.
Note that all comparisons are significant.

```{r, echo = FALSE}
lamax10 <- GetStat('pool_amax', 10)
lamax100 <- GetStat('pool_amax', 100)
lpot10 <- GetStat('pool_pot', 10)
lpot100 <- GetStat('pool_pot', 100)
```

**Table 2: Comparison of estimated precision between FloodnetAmax and FloodnetPool**

**_Q10_**
```{r}
compare(amax10$rg, lamax10$rg)
```
**_Q100_**
```{r}
compare(amax100$rg, lamax100$rg)
```

### FloodnetPool: AMAX vs POT

Next, Table 3 compares the precision RFA-AMAX and RFA-POT methods that are both performed by `FloodnetPool`.
For regions 1 to 3, the binomial and WILCOX test suggest that the precision of Q100 is not significantly different between the two methods.
For regions 5 and 6, AMAX is significantly more precise with RDP of 21.9% and 17.9%, while POT is more precise in 67.6% of the stations in region 4.  
Besides region 3, RFA-AMAX has better RDP and PCT for all regions.

**Table 3. Comparison of estimated precision between AMAX and POT when using FloodnetPool**

**_Q10_**
```{r}
compare(lamax10$rg, lpot10$rg)
```

**_Q100_**
```{r}
compare(lamax100$rg, lpot100$rg)
```

## Overall precision score

Finally, we summarize the relative precision of all methods in terms of a single score.
To do so, we rank the CIL from the least to the most precise by estimation method. 
Next, we assign a score from 0 to 3 and evaluate the average score by region.
Table 5 indicates the sum of the scores for Q10 and Q100 as an overall assessment of the modelling precision.
The resulting statistics can take values between 0 and 6. 

As expected from the pairwise comparison, the at-site methods have lower scores than the regional methods, and the at-site POT method does better than the at-site AMAX method.
Overall, the regional method using AMAX has the best precision scores for regions 1, 2, 5 and 6, while the regional POT method has the best scores for regions 3 and 4.

```{r}
sk10 <- rbind(amax10$sk, pot10$sk,  lamax10$sk, lpot10$sk)
sk100 <- rbind(amax100$sk, pot100$sk,  lamax100$sk, lpot100$sk)

sk10 <- t(apply(sk10, 2, rank))
sk100 <- t(apply(sk100, 2, rank))

Fbest <- function(z){
  y <- split(as.data.frame(z), supreg)
	ans <- (ncol(y[[1]])-sapply(y, colMeans))
	ans <- cbind(ans, all = rowMeans(ans))
	rownames(ans) <- c('AT-AMAX', 'AT-POT', 'RFA-AMAX', 'RFA-POT')
	as.data.frame(round(ans,1))
}
```

**Table 5: Comparison of all methods based on the overall precision score.**

```{r}
Fbest(sk10) + Fbest(sk100)
```

## Homogeneity of the pooling groups

The degree of homogeneity of a pooling group can be quantified by the statistic $H$ of Hosking and Wallis.
This statistic measures the variability of the Linear Coefficient of Variation (LCV).
A large value of $H$ is an indication that the hypothesis of the Index-food model used by the L-moments algorithm may not be verified.
In that sense, a conventional interpretation of the $H$ statistic is that pooling groups are homogenous when $H \leq 1$, are possibly homogeneous when $1 < H \leq 2$ and are heterogenous when $H>2$.

Table 6 presents the percentage of stations classified by the $H$ statistic according to the conventional interpretation.
For AMAX, we observe that 19.6% of the pooling groups are heterogeneous, of which 15.9 are found in super regions 3 and 4. 
On the other hand, the pooling groups formed for POT analysis are rarely heterogeneous, with a total of 2.1%.
The important proportion of heterogeneous pooling groups found with AMAX suggests that the Index-Flood model was not appropriate for a substantial number of stations.
This interpretation could explain the better overall precision scores of RFA-AMAX for super regions 3 and 4, as seen in Table 5.

```{r}
het <- read.csv(gzfile('RFA/heteo.csv.gz'))

CutH <- function(z){
	cut(z, c(-Inf,1,2, Inf), 
			labels = c('Homo.','Possibly.','Hetero.'))
}

CutN <- function(z){
	cut(z, c(0,15,20,25))
}

hamax <- CutH(het$hamax)
namax <- CutN(het$namax)
hpot <- CutH(het$hpot)
npot <- CutN(het$npot)

```

**Table 6 : Percentage of pooling by Degree of homogeneity and super regions.**

```{r}
Htab <- function(z, l){
	tb <- table(z, supreg, dnn = c(l,'Region'))
	round(100*addmargins(tb)/1114,1)
}
	
Htab(hamax, 'H_AMAX')
```
```{r}
Htab(hpot, 'H_POT')
```

## Summary graph of the at-site analysis.

The graphic below is a summary produced by `atsite_script.R`.
The left panels show the return level plots of the at-site AMAX and POT methods for the station `01AD002`.
The number of observations (N) is indicated below the x-axis. 
The top right panel presents the estimated flood quantiles with the confidence interval (95%), and the bottom right panel shows the respective coefficient of variations, _i.e._ the standard deviation divided by the flood quantile. 

<center>
  <img src="atsite_01AD002.png" height="600" width="600">
</center>

## Summary Graphic of the regional analysis.

The graphic below is a summary produced by `RFA_script.R`.
The top panels illustrate the pooling group of a target station (in red) for the seasonal and descriptor space. 
The members of the pooling group are in blue, and the black dots are the other stations. 
The bottom panels present the estimated flood quantiles with the confidence interval (95%) by return periods as well as the coefficient of variations. 

<center>
  <img src="rfa_01AD002.png" height="600" width="600">
</center>

## Super regions

We report the results by regions having similar geographical locations and site characteristics.
Figure 1 illustrates the descriptor space defined by the drainage area (AREA) and Mean Annual Precipitation (MAP).
See the folder `super_region` for a brief description.

```{r, echo = FALSE, fig.width=9, fig.height = 5}
## Set colors for displaying the clusters
mycol <- c('#e41a1c','#377eb8','#4daf4a','#984ea3','#ff7f00','#ffff33')
palette(mycol)

layout(matrix(c(1,2), 1,2))
  
coord <- GAUGEDSITES[,c('lon','lat')]

desc <- log(GAUGEDSITES[,c('area','map')])

ucol <- sort(unique(supreg))
  
sp::plot(map_ca)
title(main = 'Geographical space')
axis(1)
axis(2)

points(coord, pch = 16, col = supreg)
  
legend('top', horiz = TRUE,legend = seq_along(ucol), col = ucol, 
             pch = rep(16,12), cex = .5)
  
plot(desc, pch = 16, col = supreg, 
           xlab = 'AREA (log)',
           ylab = 'MAP (log)',
           main = 'Descriptor space')
```